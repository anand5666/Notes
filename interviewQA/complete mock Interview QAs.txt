>CAN YOU JUST TELL ME WHAT ARE ALL THE TASKS AND ONE OF THE TOOLS THAT YOU WORKED IN DEVOPS

I Have started my career with DevOps I used to work with Jenkins, maven, git and artifactory during my initial days of my career later I started working with the docker, kubernetes and ansible as well
I have also performed POC's on cloud platforms like AWS and GCP.

>WHAT IS CONTINUOUS INTEGRATION AND CONTINUOUS DELIVERY AND CONTINUOUS DEPLOYMENT 

continuous integration continuous delivery and continuous deployment are all software development or software release practices which most of the companies have adopted according to their requirements.
before they used to follow a waterfall model, agile methodologies etc those had certain issues,
like in case of waterfall model there will not be continuous feedback and 	
in case of agile we can have continuous feedback but there is no continuous integration testing.

#continuous integration is a process where developers frequently merge their code change into a shared repository then there will be automated building and testing.
#continuous delivery is actually the extension of continuous integration where we will mimic few things before production deployment 
like run current project on a fixed server test performance so only when everything is working fine then there will be a manual approval to deploy to production environment. 
#continuous deployment is the process where it validates if code changes are stable using automated tests and deploy to production without any explicit approval so changes will be directly deployed on production server.

> YOU KNOW WHAT IS GIT MERGE AND GIT REBASE

git merge and git rebase actually designed for same purpose that is to integrate changes from one branch to another branch
but they do it in different ways for example like when
I am working on a new feature in my project in a dedicated branch and then one of my team members updates development branch with new commits and
if that new commits are relevant to the feature branch that I am working on then I would need to incorporate those new commits into my feature branch
so to do this I have two options that is merge and rebase.
when I do git merge it creates a new merge commit in my feature branch that ties histories of both branches and then 
when I do git rebase it moves the entire feature branch to begin on the tip of the development branch incorporating all those new commits 
so with git rebase we can get clean project history as we get all unique commits from both branches one by one.

>DO YOU KNOW GIT SQUASH

Git squash is basically for combining commits that can be achieved with the rebase
basically that git rebase --interactive will allow us to rewrite repository history.


>YOU MENTIONED LIKE YOUR WORKED ON COUPLE OF PROJECTS RIGHT SO WHAT BRANCHING STRATEGY YOU USED  IN YOUR PROJECT AND WHICH YOU FELT LIKE VERY GOOD IN PERSPECTIVE OF BRANCHING STRATEGIES.

currently in our project we have master branch, development branch, feature branch, release and hotfix branches 
so master it contains all production code which is stable.
development branch has a pre production code like when we are assigned with a new feature to add on, then we will create branch from development branch
and when we are done with the new feature we will create pull request and then that is merged into development branch and that particular feature branch will be deleted 
release branch is to support the preparation of new production release and hotfix is just to patch production releases.

>THERE ARE BUILD LIFE CYCLES IN MAVEN RIGHT SO CAN YOU JUST LIST OUT ALL THE LIFE CYCLES

yeah sure there are 3 build life cycles in maven those were Build, clean, and site.

#The Build lifecycle handles your project deployment
Phases
validate: validate the project is correct and all necessary information is available
compile : compile the source code of the project
test    : test the compiled source code using a suitable unit testing framework. These tests should not require the code be packaged or deployed
package : take the compiled code and package it in its distributable format, such as a JAR.
verify  :run any checks on results of integration tests to ensure quality criteria are met
install : install the package into the local repository, for use as a dependency in other projects locally
deploy  :done in the build environment, copies the final package to the remote repository for sharing with other developers and projects.

#The clean lifecycle handles project cleaning
Phases
pre-clean : execute processes needed prior to the actual project cleaning
clean     : remove all files generated by the previous build
post-clean:	execute processes needed to finalize the project cleaning

The site lifecycle handles the creation of your project's web site.
Phases
pre-site	:	execute processes needed prior to the actual project site generation
site		:generate the project's site documentation
post-site	:execute processes needed to finalize the site generation, and to prepare for site deployment
site-deploy :deploy the generated site documentation to the specified web server

>WHAT DOES MVN SITE USED FOR

okay that actually generates site documentation for the project like it has 4 phases and the those are 
pre site, site, post site and site deploy
actually it will generate the project report which will include a project license, project summary about details dependencies source repository, continuous integration report and all of that stuff 
and it will create one HTML document

>OKAY YOU KNOW WE HA HAVE A COUPLE OF REPOSITORIES IN MAVEN IS LOCAL REPOSITORY REMOTE REPOSITORY CENTRAL
REPOSITORY RIGHT SO FOR THE SECOND TIME WHEN YOU RUN THE PROJECT SO IT WILL TAKE LESS TIME BECAUSE THE FIRST TIME WHEN
YOU RUN MVN INSTALL OR ANY COMMANDS LIKE IT WILL DOWNLOAD ALL THE DEPENDENCIES AND PLUGINS AND IT WILL STORE IN .M2 
LET'S SAY I I DON'T WANT TO USE THAT .M2 AS MY LOCAL REPOSITORY I NEED TO USE SOMETHING SOME OTHER FOLDER AS A LOCAL REPOSITORY SO IS IT ACHIEVABLE IN MAVEN

yes definitely we have two options can just modify settings .XML file a wherein we need to edit local repository tag with new directory 
and we can execute a mvn install - D local.repo commande 

>SO AND THEN SO YOU WOULD HAVE WORKED DEFINITELY WITH MVN DEPLOY SO I MEAN NOW IF YOU WANT TO ACHIEVE MVN DEPLOY SO WHAT ARE ALL THE SETTINGS THAT NEED TO DO

we need to define repository that we deploy to in the project sperm file, in the distribution management section we need to define ID name and URL of the repository we will deploy our artifacts to and also 
we need to define authentication information in settings .XML file

>OKAY SO WHAT DOES LIKE MEAN BY GIV's

GIVs are nothing but group ID artifact ID and version they are used to uniquely identify our project 

>WHAT IS THE PACKAGING TAG, LIKE LET'S SAY IF YOU WANT TO GENERATE ANY WAR FILE LIKE ANYTHING SO YOU JUST NEED TO GIVE THE EXTENSION RIGHT SO THE TAG FOR THAT IS PACKAGING 
SO WHAT IS THE DEFAULT VALUE FOR THAT LET'S SAY YOU NEVER DEFINE THAT IN YOUR POM IF IT IS NOT DEFINED WHAT WILL BE THE VALUE FOR THAT 

jar is the default value of packaging tag and you also have other values like POM WAR VAR JAR

>WHAT ARE THE JOB TYPES THAT YOUR WORKED IN JENKINS

I have worked mostly on a freestyle, MAVEN and pipeline jobs also have a good idea about multi configuration project, multi branch pipeline 

>YOU SAID LIKE SO YOU EXPLAINED ME THAT BRANCHING STRATEGY RIGHT SO A DEVELOPER TAKES A CODE OUT OF MASTER BRANCH AND HE CREATES A FEATURE BRANCH AND HE RAISES A PR(PULL REQUEST) TO GET IT MERGE
BACK TO MASTER RIGHT
SO I WANT TO CREATE A JOB FOR THAT THAT PR REQUEST SO IS IT POSSIBLE IN JENKINS

yeah we can have job for PR we can use multi branch pipelines job for PR 

>AND ALSO YOU DID MENTION THAT LIKE ONCE THE MERGE IS DONE SO THE BRANCH WILL BE DELETED IN THE SOURCE SO HOW THAT IS DONE

in github settings we can enable that particular option to delete the source branch when merge is done.

>YOU'VE MENTIONED THAT YOU'VE WORKED ON PIPELINE JOBS RIGHT SO THERE WILL BE SOMETHING CALLED AS POST BLOCK RIGHT SO WHY THAT IS USED FOR AND WHAT IS THE IMPORTANCE LIKE IS IT MANDATORY TO HAVE IT

since the post section of the pipeline will run at the end of pipeline's execution we can perform several tasks like a finalization, cleaning up, notification so
we have some test conditions that we can use in the post block of jenkins's like always unstable aborted successfull
that depending on the requirement we can define this test conditions like when we use always regardless of the completion status of the pipeline's run it will always run the steps in this post
condition similarly if the completion status is unstable then only it will run the steps in the post condition of the unstable so we can use these test conditions in the post block of Jenkins's

>HOW WE CAN TAKE A BACKUP IN JENKINS 
yeah I have worked on it usually all the settings, artifact archives, building logs are stored under on Jenkins home directory so we can just take backup of this Jenkins home directory and usually it will be under
.Jenkins path and also we have Jenkins backup plug-in we can which we can use it to take a backup. 

>YOU TELL ME THE VERSION OF DOCKER YOU'VE WORKED IN LIKE 


>DOCKER COMES WITH OTHER COMPONENTS LIKE CAMPOSE AND DOCKER SWARM HAVE YOU LIKE WORKED ON IT 

yeah I have worked on it they both are meant to deal with the multi container applications.
Docker compose is the tool to define multi container application and the advantage of using compose is we can define application stack in a single file which we can keep it at the root of our project repository so that if anyone else wants to contribute to the project
they can just clone the repository and start the compose application so with this docker compose definition file with a single command we can clean up everything
and the docker swam is that is same as kubernetes, it is a container orchestration tool or which allows us to manage multiple containers deployed across multiple host machines 
it allows us to perform more scaling networking and maintaining internalized applications so the difference here is a docker swam is used
to scale application across one or more servers but the docker compose will simply run application on a single host.

>WHY WE NEED DOCKER VOLUME

when we use volume command it will actually create a new folder in the host system so when that particular container crashes we can attach the same volume to other container and we
use bind mounts to bind an existing folder in the host system to a path in the container so when we link development folders to a path in the Container so any change in the host folder will reflect in the container
so we don't have to redeploy again and again.

>HAVE YOU COME ACROSS DOCKER IGNORE FILE

yeah we can specify patterns for files and folders or to exclude from final build image so when we do docker build it is necessary to prevent files from being added to build context so it
avoids some unnecessary sending large and sensitive files to the image build

>IS IT POSSIBLE TO NAME A DOCKER FILE AS TEST.TXT OR PROJECTNAME.TXT SOMETHING LIKE THAT
yeah we can name docker files however we like our default file name is Docker file without any extension 
but if we use default name it makes various tasks easier like when use build command we can simply just specify .(dot) to refer docker file in that location 
so if we name our docker file with any other name we should specify using the -f when building image.


darker system prune - with this command we can delete all stop containers and unused images

WHEN YOU CREATE A DEPLOYMENT SO ALL THE PODS ARE ANYTHING WILL GO ON OTHER MISSIONS LIKE WORKER NODES
SO CAN WE DEPLOY ON A MASTER NODE

yes we can deploy on master node but the best practice is not to deploy application workloads on a master server 
when the kubernetes cluster is first set up we can notice tent is set on master node, so we can run kubectl describe node and you just look for
ten paint section it would be set to not schedule any load on master so but we can set dual parts on master by applying toleration to PODs with not contains 

>CONFIG MAPS

It is an object that is used to store data any key value pairs or data in the sense which is not actually sensitive so we can separate environment specific configuration from application code
using config map like when we want to change the configuration for example with respect to environment variable we can simply change it in config map file and then we are good to go we don't have to change
in every location where data is referenced in application.

>DEFAULT DEPLOYMENT STRATEGY IN KUBENETES

Rolling update strategy is actually the default deployment strategy so basically there are two types of deployment strategies recreate and rolling update okay so in case of
rolling update application never goes down we will not destroy all the instances at once we will actually take take down older version and bring up a newer version one by one 
but in case of recreate strategy we will destroy all instances at once and create newer versions so in this case application will be down and inaccessible to the users so default strategy is rolling update 

>SO WHICH ONE DO YOU FEEL IS EFFECTIVE LIKE LET'S SAY IT IS

okay in production rolling updaters better but in development I think we can go with both are fine

>HAVE YOU USED HELM IN YOUR PROJECT 

yeah yeah like I deploy application using helm chart 

>OKAY SO WHY I MEAN EXACTLY WHY WE NEED HELM

helm is basically a package manager for kubernetes. it will actually ease the deployment or complexity with kubernetes which involves writing many interdependent kubernetes resources such
as deployment services, secrets, config maps etc, so it enables to deploy application easily with single helm install command instead of creating every resource separately which will also improves developer productivity.


>OKAY SO IN HELM SO BASICALLY WE WILL DEAL WITH MANY LIKE FILES WE'LL HAVE
TEMPLATES AND VALUES.YML AND MANYTHINGS RIGHT SO MY REQUIREMENT IS LIKE I'VE DEFINED MANY VALUES LIKE IN  VALUES.YML, I HAVE DEFINED MANY TAGS
BUT I'M NOT USING IN MY TEMPLATES SO IS THERE ANY WAY BY WHICH WE CAN SEE WHICH IS NOT UNUSED YML TAGS  
I MEAN IF NOT LIKE CAN WE WRITE A SHELL SCRIPTING FOR THAT

yeah we can write script for that also we have one more option like we can just simply
run helm linked -- detect unused values 
so with that I think we can just identify what are all the unused fields and values.yml 

>HAVE COME ACROSS EXIT STATUS AND WHY EXACTLY WE USE THAT EXIT STATUS
yeah it is something that every command returns and it is know whether the command is success or failure this is used and for a successful command it returns zero and
for an unsuccessful one it just returns a non zero value and this dollar question mark we used to know that exit status.

>IS THERE ANY COMMAND TO IDENTIFY whats OS IS running in a machine whethere it is UBUNTU/CENT OR ANYTHING
gcc --version

we can just execute uname -a 

>CAN WE EXECUTE ANY PLAYBOOK HAVING DIFFERENT OSS LIKE LET'S SAY I WANT TO EXECUTE SOME TASKS ON UBUNTU AND SOME TASKS ON CENT OS 

yes we can mention that host in inventory file and we can execute it

>ANSIBLE GALAXY WHY IT IS USED.

it is actually a public repository for ansible rules from where we can use them in our playbooks so it is just a free site for finding ,downloading sharing
rules which we can use for certain tasks like infrastructure, deploying application and all certain tasks that we do every day.

>have you worked on ADHOC commands

ADHOC commands are one liner commands, when we have tasks that we don't repeat on daily basis which we pick very rarely for those adhoc commands are the best.
that work quick and easy but they can't be reused. so common example that I can give is like when we want to power off several machines in lap for several days mm-hmm
we can execute a quick one liner command to perform that task without writing the playbook.

>CAN WE STORE THE OUTPUT OF A COMMAND in the form of text or something

i think we can by using > symbol followed by destination path

HANDLERS AND NOTIFY IN ANSIBLE PLAY BOOKS

handlers are regular tasks in PlayBook but it runs only if the tasks contain notify directive and indicates that it changed something for example if the
configuration file is changed then the tasks referencing that configuration file may notify a service restart handler so handlers will perform an
action when listens for a notify event nothing notifies a handler, it will not run.

>WHAT ARE ALL THE SERVICES THAT YOU HAVE WORKED ON AWS

I have worked on a few services like EC2, IAM, Route 53, EBS, cloudwatch

